{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МФТИ ФИВТ: Курс Машинное Обучение (осень, 2016), Арсений Ашуха, ars.ashuha@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Organization Info</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительный материал для выполнения дз**:\n",
    "- Hastie, The Elements of Statistical Learning, https://goo.gl/k3wfEU\n",
    "    - 2.9 Model Selection and the Bias–Variance Tradeoff \n",
    "    - 15 Random Forests\n",
    "- Соколов, Семинары по композиционным методам, https://goo.gl/sn8RyJ\n",
    "- Andrew Ng, Bias vs. Variance, https://goo.gl/1ISZ6Y\n",
    "\n",
    "**Оформление дз**: \n",
    "- Присылайте выполненное задание на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2016_fall <номер_группы> <фамилия>``, к примеру -- ``ML2016_fall 401 ivanov``\n",
    "- Выполненное дз сохраните в файл ``<фамилия>_<группа>_task<номер>.ipnb``, к примеру -- ``ivanov_401_task1.ipnb``\n",
    "\n",
    "**Вопросы**:\n",
    "- Присылайте вопросы на почту ``ml.course.mipt@gmail.com``\n",
    "- Укажите тему письма в следующем формате ``ML2016_fall Question <Содержание вопроса>``\n",
    "\n",
    "--------\n",
    "- **PS1**: Мы используем автоматические фильтры, и просто не найдем ваше дз, если вы не аккуратно его подпишите.\n",
    "- **PS2**: Напоминаем, что дедлайны жесткие, письма пришедшие после автоматически удаляются =( чтобы соблазна не было "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответе на вопросы своими словами (загугленный материал надо пересказать), ответ обоснуйте (напишите и ОБЪЯСНИТЕ формулки если потребуется), если не выходит, то вернитесь к лекции дополнительным материалам:\n",
    "\n",
    "**Вопрос 1**: Какие формулы у шума, смещения, разброса? Какой смысл у этих компонент?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "\n",
    "Здесь показано, что среднеквадратичный риск раскладывается на 3 компоненты.\n",
    "$\\sigma^2$ -- это неустранимая ошибка, возникающая из-за шума в данных, характеризующемся здесь $\\epsilon$.\n",
    "Bias -- смещение, среднее отклонение алгоритма от правильного ответа.\n",
    "Variance -- разброс, характеризует вариативность ответа алгоритма. Разброс ответов обученных алгоритмов относительно среднего ответа.\n",
    "\n",
    "$$y = f(x) + \\epsilon$$\n",
    "$$\\mathrm{E}\\Big[\\big(y - \\hat{f}(x)\\big)^2\\Big] = \\mathrm{Bias}\\big[\\hat{f}(x)\\big]^2 + \\mathrm{Var}\\big[\\hat{f}(x)\\big] + \\sigma^2 \\\\$$\n",
    "\n",
    "---------\n",
    "$$ \\mathrm{Bias}\\big[\\hat{f}(x)\\big] = \\mathrm{E}\\big[\\hat{f}(x) - f(x)\\big] $$\n",
    "\n",
    "\n",
    "and\n",
    "\n",
    "$$\\mathrm{Var}\\big[\\hat{f}(x)\\big] = \\mathrm{E}[\\hat{f}(x)^2] - \\mathrm{E}[\\hat{f}(x)]^2$$\n",
    "\n",
    "\n",
    "**Вопрос 2**: 4. Приведите пример семейства с маленьким смещением и большим разбросом. Приведите пример семейства с большим смещением и маленьким разбросом.\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "* Линейные алгоритмы -- большое смещение, маленький разброс;\n",
    "* Деревья -- большой разброс (потому что сильно меняется дерево в зависимости от каждой новой выборки данных), маленькое смещение;\n",
    "\n",
    "**Вопрос 3**: Как сгенерировать подвыборку с помощью бутстрапа?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "Нужно выбрать n объектов с возвращением. Т.е. в итоговой выборке могут быть одни и те же объекты по нескольку раз.\n",
    "\n",
    "**Вопрос 4**: Что такое бэггинг?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "Это метод составления композиции алгоритмов. Он заключается в независимом обучении нескольких алгоритмов на бутстрапированных выборках, а потом последующее усреднение их ответов.\n",
    "\n",
    "**Вопрос 5**:  Как соотносятся смещение разброс композиции, построенной с помощью бэггинга, со смещением и разбросом одного базового алгоритма?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "Смещение не меняется, разброс уменьшается в n раз, где n -- число алгоритмов в композиции (конечно, если алгоритмы не скоррелированы).\n",
    "\n",
    "**Вопрос 6**: Как обучается случайный лес? В чем отличия от обычной процедуры построения решающих деревьев?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "При построении дерева для случайного леса использьзуются бутстраппированные выборки данных, при этом разделение объектов в каждой вершине происходит путем выбора признака не из всех признаков сразу, а только из некоторой случайной подвыборки.\n",
    "\n",
    "\n",
    "**Вопрос 7**: Почему хорошими базовыми алгоритмами для бэггинга являются именно деревья?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "Потому что они достаточно сложны, чтобы у них было маленькое смещение, но они сильно переобучаются, из-за чего имеют большой разброс.\n",
    "Т.к. бэггинг уменьшает разброс базовых алгоритмов, то вот. Все хорошо.\n",
    "\n",
    "\n",
    "**Вопрос 8**: Как оценить качество случайного леса с помощью out-of-bag-процедуры?\n",
    "\n",
    "<Ответ>\n",
    "\n",
    "Для каждого объекта найти деревья, которые на нем не обучались. Посчитать средний ответ для каждого объекта по всем таким деревьям и вычислить функцию потерь от этого среднего ответа. Далее просуммировать по всем объектам результаты вычисления функции потерь.\n",
    "\n",
    "\n",
    "\n",
    "-----------\n",
    "PS: Если проверяющий не понял ответ на большинство вопросов, то будет пичалька. Пишите так, чтобы можно было разобраться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Bagging</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Известно, что бэггинг плохо работает, если в качестве базовых классификаторов взять knn. Попробуем понять причины на простом примере.\n",
    "\n",
    "Пусть дана выборка $X^l$ из $l$ объектов с ответами из множества $Y = \\{−1, +1\\}$. Будем рассматривать классификатор одного ближайшего соседа в качестве базового алгоритма. Построим с помощью бэггинга композицию длины $N$:\n",
    "\n",
    "$$a_N(x) = sign(\\sum_{n=1}^{N} b_n(x))$$\n",
    "\n",
    "Оцените вероятность того, что ответ композиции на произвольном объекте x будет\n",
    "отличаться от ответа одного классификатора ближайшего соседа, обученного по всей\n",
    "выборке. Покажите, что эта вероятность стремится к нулю при N → ∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.nanmean([1,2,3, np.nan, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**<Решение>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Bagging Implementation</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте беггинг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "class BaggingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_estimator, n_estimators, items_rate=1.0, features_rate=1.0):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        base_estimator: sklearn.Classifier\n",
    "            Базовый алгоритм, который можно обучить (есть метод fit).\n",
    "            Для обучение композиции нужно много таких, можэно получить с помощю copy.deepcopy\n",
    "\n",
    "        n_estimators: int\n",
    "            Число алгоритмов в композиции\n",
    "\n",
    "        items_rate: float > 0\n",
    "            Доля объектов из трейна, на которой будет обучаться каждый базовый алгоритм\n",
    "\n",
    "        features_rate: float > 0\n",
    "            Доля фичей, на которой будет обучаться и применяться каждый базовый алгоритм\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.items_rate = items_rate\n",
    "        self.features_rate = features_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Метод должен обучить композицию алгоритмов, используя X, y как обучающую выборку.\n",
    "        Не забудте реализорвать функционал выбора случайных объектов и фичей.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array\n",
    "        y: 1d np.array\n",
    "        \"\"\"\n",
    "        def random_choice(X, rate):\n",
    "            \"\"\"\n",
    "             Метод возвращает подвыборку,\n",
    "             в соотношении rate к общему числу объектов\n",
    "             если 1D массив. Иначе возвращает набор индексов.\n",
    "             \n",
    "            \"\"\"\n",
    "            if type(X) is csr_matrix:\n",
    "                n = int(np.round(rate*X.shape[0],0))\n",
    "                X_return = np.random.choice(range(X.shape[0]), n)\n",
    "            else:\n",
    "                n = int(np.round(rate*len(X),0))\n",
    "                if len(np.array(X).shape) == 1:\n",
    "                    X_return = np.random.choice(X, n)\n",
    "                else:\n",
    "                    X_return = np.random.choice(range(len(X)), n)\n",
    "            return X_return\n",
    "            \n",
    "\n",
    "        # Тут храните обученные базовые алгоритмы\n",
    "        self.estimators = []\n",
    "\n",
    "        # Тут храните фичи для каждого алгоритма\n",
    "        self.features_idx = []\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            estimator = deepcopy(self.base_estimator)\n",
    "            # =======================================\n",
    "            # Обучите базовые алгоритмы\n",
    "            # =======================================\n",
    "            inds = random_choice(X, self.items_rate)\n",
    "            features_inds = random_choice(range(X.shape[1]), self.features_rate)\n",
    "            X_sampled = X[inds,:][:,features_inds]\n",
    "            y_sampled = y[inds]\n",
    "            \n",
    "            self.features_idx.append(features_inds)\n",
    "            self.estimators.append(estimator.fit(X_sampled, y_sampled))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: 2d np.array матрица объекты признаки на которых нужно сказать ответ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y_pred: 1d np.array, Вектор классов для каждого объекта\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        probs = [] # Храните тут ответы каждого базового алгоритма \n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            # =======================================\n",
    "            # Получите ответы (вероятности) от всех базовых алгоритмов\n",
    "            # ======================================\n",
    "            values = self.estimators[i].predict(X[:,self.features_idx[i]])\n",
    "            probs.append(values)\n",
    "\n",
    "        # =======================================\n",
    "        # Усредните вероятности полученные от базовых алгоритмов\n",
    "        # =======================================\n",
    "        y_pred = map(int,np.round(np.mean(probs, axis = 0),0))\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "titanic = pd.read_csv('./train.csv')[['Survived', 'Pclass', 'Sex', 'Age', 'Fare']]\n",
    "\n",
    "sex_encoder = LabelEncoder()\n",
    "titanic.Sex = sex_encoder.fit_transform(titanic.Sex)\n",
    "features = ['Pclass', 'Sex', 'Age', 'Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = titanic[features].values, titanic.Survived.values\n",
    "X = np.nan_to_num(X)\n",
    "X_train, y_train, X_test, y_test = X[:500], y[:500], X[500:], y[500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нужно обучить свой беггинг на датасете титаник, и посмотреть работает ли он. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844 0.74168797954\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 10 моделями\n",
    "# =======================================\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(), 10, 0.4, 0.4)\n",
    "clf.fit(X_train, y_train)\n",
    "print accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите эксперименты:\n",
    "    - Работает-ли беггинг лучше чем просто линейная модель?\n",
    "    - Какой items_rate и features_rate работает лучше и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': 0.29999999999999999, 'feat': 0.90000000000000002}\n",
      "0.823529411765\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier с 100 моделями\n",
    "# =======================================\n",
    "rate = np.array(range(3,10))/10.0\n",
    "best_acc = 0.5\n",
    "best_pair = {\"items\":1, \"feat\":1}\n",
    "\n",
    "for feat_r in rate:\n",
    "    for item_r in rate:\n",
    "        clf = BaggingClassifier(DecisionTreeClassifier(), 100, item_r, feat_r,)\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc = accuracy_score(clf.predict(X_test), y_test)\n",
    "        if acc>best_acc:\n",
    "            best_pair[\"items\"] = item_r\n",
    "            best_pair[\"feat\"] = feat_r\n",
    "            best_acc = acc\n",
    "print(best_pair)\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.80000000000000004, 0.76982097186700771)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# =======================================\n",
    "# Обучите LogsiticRegression \n",
    "# =======================================\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "acc = accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да, бэггинг работает лучше, чем просто линейная модель, что видно по скору на тесте.\n",
    "\n",
    "Наилучшими кажутся feature_rate = 0.9, items_rate = 0.3 из эксперимента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adult = pd.read_csv(\n",
    "    './data/adult.data', \n",
    "    names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"], \n",
    "    header=None, na_values=\"?\")\n",
    "\n",
    "adult = pd.get_dummies(adult)\n",
    "adult[\"Target\"] = adult[\"Target_ >50K\"]\n",
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values\n",
    "X_train, y_train, X_test, y_test = X[:20000], y[:20000], X[20000:], y[20000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответте на вопросы:\n",
    "    - Работает-ли беггинг лучше чем просто линейная модель?\n",
    "    - Какой items_rate и features_rate работает лучше и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': 0.29999999999999999, 'feat': 0.90000000000000002}\n",
      "0.854310962503\n"
     ]
    }
   ],
   "source": [
    "rate = np.array(range(3,10))/10.0\n",
    "best_acc = 0.5\n",
    "best_pair = {\"items\":1, \"feat\":1}\n",
    "\n",
    "for feat_r in rate:\n",
    "    for item_r in rate:\n",
    "        clf = BaggingClassifier(DecisionTreeClassifier(), 10, item_r, feat_r,)\n",
    "        clf.fit(X_train, y_train)\n",
    "        acc = accuracy_score(clf.predict(X_test), y_test)\n",
    "        if acc>best_acc:\n",
    "            best_pair[\"items\"] = item_r\n",
    "            best_pair[\"feat\"] = feat_r\n",
    "            best_acc = acc\n",
    "print(best_pair)\n",
    "print(best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.79990000000000006, 0.79515962104927951)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# =======================================\n",
    "# Обучите LogsiticRegression \n",
    "# =======================================\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "acc = accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видимо, бэггинг все-таки лучше.\n",
    "\n",
    "'items': 0.3, 'feat': 0.9 про rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Text, Image Classification</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дальше в каждом эксперименте нужно: \n",
    "- сравниться с линейной моделью ( какую лучше выбрать?=) )\n",
    "- сделать выбор в пользу одной из моделей\n",
    "- выбор обосновать, почему одна из моделей хуже а другая лучше\n",
    "- что такое хуже и лучше\n",
    "- попробуйте беггинг над деревьями и линейными моделями \n",
    "- почему работает или не работает, какие особенности данных на это влияют"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sklearn.datasets.twenty_newsgroups:Downloading dataset from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train, y_train = vectorizer.fit_transform(newsgroups_train.data), newsgroups_train.target\n",
    "X_test,  y_test  = vectorizer.transform(newsgroups_test.data), newsgroups_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge 0.996906487538 0.853425385024\n",
      "svm 0.999027753226 0.853159851301\n",
      "logist 0.969860350009 0.827934147637\n",
      "sgd 0.993282658653 0.850902814657\n",
      "CPU times: user 36.5 s, sys: 384 ms, total: 36.8 s\n",
      "Wall time: 37.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите Линейную модель \n",
    "# =======================================\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "logist = LogisticRegression()\n",
    "ridge = RidgeClassifier()\n",
    "sgd = SGDClassifier()\n",
    "svm = LinearSVC()\n",
    "dict_meth = {\"logist\": logist, \"ridge\": ridge, \"sgd\": sgd, \"svm\": svm}\n",
    "\n",
    "for meth in dict_meth.keys():\n",
    "    clf = dict_meth[meth].fit(X_train, y_train)\n",
    "    print meth, accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем для сравнения ridge clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3682163691 0.215215082315\n",
      "CPU times: user 2min 18s, sys: 1.36 s, total: 2min 19s\n",
      "Wall time: 2min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier\n",
    "# =======================================\n",
    "\n",
    "\n",
    "clf = BaggingClassifier(DecisionTreeClassifier(), 10, 0.5, 0.9)\n",
    "clf.fit(X_train, y_train)\n",
    "print accuracy_score(clf.predict(X_train), y_train), accuracy_score(clf.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не знаю почему же так плохо..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils_ import load_cifar10\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10('./data/cifar10')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(X_train.shape[0], -1), X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите Линейную модель \n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# =======================================\n",
    "# Обучите беггинг над DecisionTreeClassifier\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Random Forest Feature Impotance</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Опишите как вычисляется важность фичей в дереве, можете изучить как работает  feature\\_importances_ в sklearn.\n",
    "\n",
    "---\n",
    "\n",
    "<Ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Почитайте Feature Impotance для Adult и Titanic (используйте полный датасет), ПРОИНТЕРПРЕТИРУЙТЕ резульататы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values\n",
    "X_train, y_train, X_test, y_test = X[:20000], y[:20000], X[20000:], y[20000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, \n",
    "                        items_rate=1, features_rate=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# Посчитайте feature_importances для clf\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Titanick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = titanic[features].values, titanic.Survived.values\n",
    "X = np.nan_to_num(X)\n",
    "X_train, y_train, X_test, y_test = X[:500], y[:500], X[500:], y[500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, \n",
    "                        items_rate=1, features_rate=1).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# =======================================\n",
    "# Посчитайте feature_importances для clf\n",
    "# ======================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

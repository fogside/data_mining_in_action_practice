{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МФТИ ФИВТ: Курс Машинное Обучение (осень, 2016), Арсений Ашуха, ars.ashuha@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Check Questions</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вопрос 1**: Зачем нужно структурное предсказание?\n",
    "\n",
    "см. вопрос 6\n",
    "\n",
    "**Вопрос 2**: Что такое сопряженное распределение?\n",
    "\n",
    "Распределения $p(X|\\theta)$ и $p(\\theta)$ называются сопряженными, если $p(\\theta)$ и $p(\\theta|X)$ принадлежат к одному семейству распределений.\n",
    "\n",
    "**Вопрос 3**: Какое распределение сопряженное к равномерному, докажите?\n",
    "\n",
    "**Ответ:** Распределение парето.\n",
    "\n",
    "$P(X|\\theta) = \\frac{1}{\\theta^n} \\cdot I(0 \\leq x_1,...,x_n \\leq \\theta)$\n",
    "\n",
    "Далее убедимся, что сопряженным распределением является распределение Парето:\n",
    "Пусть $P(\\theta | a,b) = I(\\theta \\geq a) \\frac{ba^b}{\\theta^{b+1}}$, \n",
    "тогда $P(X|\\theta) \\cdot P(\\theta|a,b) = I(X_{(n)} \\leq \\theta, a \\leq \\theta)\\frac{1}{\\theta^n} \\frac{1}{\\theta^{b+1}} \\cdot const = I(max(X_{(n)},a) \\leq \\theta) \\frac{1}{\\theta^{b+1 + n}} \\cdot const,$\n",
    "\n",
    "**Вопрос 4**: В чем заключается Байесовский подход к машинному обучению?\n",
    "\n",
    "Мы говорим, что на самом деле у нас есть какое-то вероятностное распределение на $X\\times Y$, где $X$ - пространство сэмплов, $Y$ - пространство ответов. Мы хотим найти смесь распределений, которая достаточно хорошо приближает данное. Таким образом мы сможем делать пресказания - например, как наиболее вероятное значение $y$. \n",
    "\n",
    "**Вопрос 5**: В чем основные преимущества Байесовского подхода к машинному обучения?\n",
    "\n",
    "Во-первых, на выходе мы получаем распределение. Во-вторых, при применении байесовского подхода мы можем использовать для каждого признака свой регуляризотор и получить тем самым некий отбор признаков. К тому же, т.к. у нас есть распределение на ответах, мы, выдавая конкретный ответ для сэмпла, можем также сказать, насколько мы в нем уверены\n",
    "\n",
    "**Вопрос 6**: Чем отличается структурный метод опорных векторов от неструктурного, в чем сложности при обучении структурного?\n",
    "\n",
    "S-SVM подходит для более широкого класса задач - он может использоваться в задаче, в которой просто есть множество структурированных ответов (т.е. он умеет делать структурные предсказания). Например, если мы делаем разбор предложения, то S-SVM даст нам ответ из множества всех возможных деревьев разбора данного предложения. Т.е. целевые переменные не ограничиваются номерами классов или действительными числами (в случае регрессии). В обычном SVM мы смотрим на $<w, \\psi(x)y>$, в S-SVM - на $<w, \\psi(x,y)>$. $\\psi$ - некоторая функция. При этом, проблема при обучении S-SVM состоит в том, что данная функция может быть произвольной, а значит решение оптимизационной задачи может быть достаточно сложным\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Task</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим задачу построения коллажа из нескольких изображений, к примеру несколько фотографий группы людей, на каждой кто-то отвернулся или моргнул ... Вы хотите получить одну фотографию, на которой все получились хорошо. На вход поступает K изображений а на выходе вам нужно выдать матрицу размером с изображение, где в каждом пикселе будет указано из какой картинки вам нужно его взять. Вы хотите сделать фотографию так, чтобы места склейки были незаметны. \n",
    "\n",
    "- Введите граф модель, почему вы выбрали именно такую, приложите рисунок\n",
    "- Потенциалы каких порядков вы собираетесь использовать? \n",
    "- Определите потенциалы -- какой в них физический смысл? почему они поощряют незаметные склеивания?\n",
    "- Предложить несколько вариантов выбора потенциалов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.machinelearning.ru/wiki/images/d/db/GM16_assignment2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Для начала к вопросу о том, почему именно такая граф модель. Мы склеиваем несколько картинок и при этом хотим учитывать каждый пиксель не сам по себе, а со своими соседями. Что немаловажно MRF является граф моделью с неориентированным графом (это еще одна причина, по которой она подходит для склейки изображений)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Будем использовать потенциалы первого и второго порядков, что написано в формуле"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- По поводу физического смысла потенциалов. https://courses.cs.washington.edu/courses/cse590st/04sp/slides/mn.pdf\n",
    "![](potentials_2.png)\n",
    "![](potentials_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вот что тут написано по поводу потенциалов. По факту каждый унарный потенциал показывает, насколько данный класс свойственен конкретно данному пикселю, а потенциалы второго порядка показывают насколько вероятно наличие такой пары меток у данных двух пикселей. Как правило потенциалы штрафуют нас за выбор классов пикселей в перемешку.<br>\n",
    "Второй же скрин содержит описание того, как мы строим наш граф(это к первому вопросу): мы сопоставляем каждому пикселю k вершин, где k - количество картинок. Потом мы просто соединяем вершины, соответствующие нашему пикселю с вершинами, соответствующие его соседним пикселям (будем брать четырех соседей, как было отмечено выше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Варианты выбора потенциалов (http://www.machinelearning.ru/wiki/images/6/61/MRFtutorial2.pdf)\n",
    "\n",
    "Для унарного потенциала:\n",
    "1. Цветовая модель - показывает насколько появление тех или иных цветов более вероятно в данном классе\n",
    "2. Позиционная модель - показывает априорные предположения о положении данного класса на изображении. Т.е. насколько данный класс вероятен в данной части изображения\n",
    "3. Текстурная модель - показывает насколько текстура окрестности пикселя вероятна для данного пикселя\n",
    "\n",
    "Для бинарных потенциалов:\n",
    "1. Модель Поттса: штраф за несовпадение классов соседних пикселей. Т.е. мы не учитываем, для каких конкретно пикселей изображения мы считаем потенциал, нам важны лишь их классы $\\ \\ \\theta_{ij}(x_i, x_j) = 1 - \\delta(x_i, x_j)$\n",
    "2. Штраф за несовпадение классов с учетом контраста $\\ \\ \\theta_{ij}(x_i,x_j)=exp\\left(\\frac{-(x_i-x_j)^2}{2 \\sigma^2}\\right)(1-\\delta(x_i,x_j))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1 align=\"center\">Bonus part: Semantic Image Segmentation</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Если вы, хотите разобраться с граф моделями -- попробуйте реализовать это на питон, выполнение займет около 30-40 часов, на щедрые бонусные баллы (3-5 баллов) -- можно обсудить заранее. Если решитесь, напишите мне. \n",
    "\n",
    "http://www.machinelearning.ru/wiki/index.php?title=Графические_модели_(курс_лекций)/2012/Задание_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
